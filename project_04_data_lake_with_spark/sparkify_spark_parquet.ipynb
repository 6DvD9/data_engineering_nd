{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ecd3a58e6524a7bb235b38e4159258e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>3</td><td>application_1576766881850_0004</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-172-31-51-73.us-west-2.compute.internal:20888/proxy/application_1576766881850_0004/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-172-31-51-73.us-west-2.compute.internal:8042/node/containerlogs/container_1576766881850_0004_01_000001/livy\">Link</a></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0929f32708b4a2e942252d9318a0961",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf, col, monotonically_increasing_id\n",
    "from pyspark.sql.functions import year, month, dayofmonth, hour, weekofyear, date_format\n",
    "from pyspark.sql.types import StructType, StructField, DoubleType, StringType, IntegerType, DateType, TimestampType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e101ad1dc5cc4355b74fb0163cf46ed7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def create_spark_session():\n",
    "    \"\"\"Function to retrieve or create a spark session\"\"\"\n",
    "        \n",
    "    spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:2.7.0\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "    return spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e936b14abd5b443e941c0463e3b4c4a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def process_song_data(spark, input_data, output_data):\n",
    "    \"\"\"\n",
    "    Function that loads song and artist data (JSON format) from S3, \n",
    "    processes them by extracting songs and artist tables, and then loading back into S3\n",
    "\n",
    "    Parameters:\n",
    "        spark       : Spark Session\n",
    "        input_data  : Location of song_data in JSON format\n",
    "        output_data : Location of S3 bucket where tables are to be stored in parquet format\n",
    "    \"\"\"\n",
    "\n",
    "    # JSON structure to spark\n",
    "    song_schema = StructType([\n",
    "        StructField(\"artist_id\", StringType()),\n",
    "        StructField(\"artist_latitude\", DoubleType()),\n",
    "        StructField(\"artist_location\", StringType()),\n",
    "        StructField(\"artist_longitude\", DoubleType()),\n",
    "        StructField(\"artist_name\", StringType()),\n",
    "        StructField(\"duration\", DoubleType()),\n",
    "        StructField(\"num_songs\", IntegerType()),\n",
    "        StructField(\"title\", StringType()),\n",
    "        StructField(\"year\", IntegerType()),\n",
    "    ])\n",
    "    \n",
    "    # Get filepath to song data file\n",
    "    song_data = input_data + 'song_data/*/*/*/*.json'\n",
    "\n",
    "    # Read song data file\n",
    "    df = spark.read.json(song_data, schema=song_schema)\n",
    "\n",
    "    # Extract columns to create songs table\n",
    "    songs_table = df.select(\"title\", \"artist_id\", \"year\", \"duration\").dropDuplicates() \\\n",
    "            .withColumn(\"song_id\", monotonically_increasing_id())\n",
    "\n",
    "    # Write songs table to parquet files partitioned by year and artist\n",
    "    songs_table.write.partitionBy(\"year\", \"artist_id\").parquet(output_data + 'songs/')\n",
    "\n",
    "    # Extract columns to create artists table\n",
    "    artist_fields = [\"artist_id\", \"artist_name\", \"artist_location\", \"artist_latitude\", \"artist_longitude\"]\n",
    "    artists_table = df.select(artist_fields).dropDuplicates()\n",
    "\n",
    "    # Write artists table to parquet files\n",
    "    artists_table.write.parquet(output_data + 'artists/')\n",
    "\n",
    "    print('songs_table\\n', songs_table)\n",
    "    print('\\n')\n",
    "    print('artists_table\\n', artists_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ee94f3a0b9048b9a722088465c45536",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def process_log_data(spark, input_data, output_data):\n",
    "    \"\"\"\n",
    "    Function that loads log data (JSON format) from S3, \n",
    "    processes them by extracting songs and artist tables, and then loading back into S3.\n",
    "    Output from process_song_data function is used as an input for this function.\n",
    "\n",
    "    Parameters:\n",
    "        spark       : Spark Session\n",
    "        input_data  : Location of log_data in JSON format\n",
    "        output_data : Location of S3 bucket where tables are to be stored in parquet format\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Get filepath to log data file\n",
    "    log_data = input_data + 'log-data/*.json'\n",
    "\n",
    "    # Read log data file\n",
    "    df = spark.read.json(log_data)\n",
    "\n",
    "    # Filter by actions for song plays\n",
    "    df = df.filter(df.page == 'NextSong')\n",
    "\n",
    "    # Extract columns for users table\n",
    "    user_fields = [\"userId as user_id\", \"firstName as first_name\", \"lastName as last_name\", \"gender\", \"level\"] \n",
    "    users_table = df.selectExpr(user_fields).dropDuplicates()\n",
    "\n",
    "    # Write users table to parquet files\n",
    "    users_table.write.parquet(output_data + 'users/')\n",
    "\n",
    "    # Get datetime \n",
    "    get_datetime = udf(date_convert, TimestampType())\n",
    "    df = df.withColumn(\"start_time\", get_datetime('ts'))\n",
    "\n",
    "    # Extract columns to create time table\n",
    "    time_table = df.select(\"start_time\").dropDuplicates() \\\n",
    "            .withColumn(\"hour\", hour(col(\"start_time\"))) \\\n",
    "            .withColumn(\"day\", day(col(\"start_time\"))) \\\n",
    "            .withColumn(\"week\", week(col(\"start_time\"))) \\\n",
    "            .withColumn(\"month\", month(col(\"start_time\"))) \\\n",
    "            .withColumn(\"year\", year(col(\"start_time\"))) \\\n",
    "            .withColumn(\"weekday\", date_format(col(\"start_time\"), 'E'))\n",
    "\n",
    "    # Write time table to parquet files partitioned by year and month\n",
    "    time_table.write.partitionBy(\"year\", \"month\").parquet(output_data + 'time/')\n",
    "\n",
    "    # Read in song data to use for songplays table\n",
    "    df_songs = spark.read.parquet(output_data + 'songs/*/*/*')\n",
    "\n",
    "    df_artists = spark.read.parquet(output_data + 'artists/*')\n",
    "\n",
    "    songs_logs = df.join(songs_df, (df.song == songs_df.title))\n",
    "\n",
    "    artists_songs_logs = songs_logs.join(df_artists, (songs_logs.artist == df_artists.artist_name))\n",
    "\n",
    "    songplays = artists_songs_logs.join(\n",
    "        time_table,\n",
    "        artists_songs_logs.ts == time_table.start_time, 'left'\n",
    "    ).drop(artists_songs_logs.year)\n",
    "\n",
    "    # Extract columns from joined song and log datasets to create songplays table \n",
    "    songplays_table = songplays.select(\n",
    "        col('start_time').alias('start_time'),\n",
    "        col('userId').alias('user_id'),\n",
    "        col('level').alias('level'),\n",
    "        col('song_id').alias('song_id'),\n",
    "        col('artist_id').alias('artist_id'),\n",
    "        col('sessionId').alias('session_id'),\n",
    "        col('location').alias('location'),\n",
    "        col('userAgent').alias('user_agent'),\n",
    "        col('year').alias('year'),\n",
    "        col('month').alias('month'),\n",
    "    ).repartition(\"year\", \"month\")\n",
    "\n",
    "    # Write songplays table to parquet files partitioned by year and month\n",
    "    songplays_table.write.partitionBy(\"year\", \"month\").parquet(output_data + 'songplays/')\n",
    "\n",
    "    print('users_table\\n', users_table)\n",
    "    print('\\n')\n",
    "    print('time_table\\n', time_table)\n",
    "    print('\\n')\n",
    "    print('songplays_table\\n', songplays_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88ab7721c4be49369e1fc8bb1d354b56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def main():\n",
    "    \"\"\"Function that uses all functions and runs as main function\"\"\"\n",
    "\n",
    "    spark = create_spark_session()\n",
    "    input_data = \"s3a://udacity-dend/\"\n",
    "    output_data = \"s3a://udacity-dend/\"\n",
    "\n",
    "    process_song_data(spark, input_data, output_data)    \n",
    "    process_log_data(spark, input_data, output_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a9f16a7b3f8412c93c6a48c2e56e795",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error occurred while calling o109.parquet.\n",
      ": java.nio.file.AccessDeniedException: s3a://udacity-dend/songs/_temporary/0: innerMkdirs on s3a://udacity-dend/songs/_temporary/0: com.amazonaws.services.s3.model.AmazonS3Exception: Access Denied (Service: Amazon S3; Status Code: 403; Error Code: AccessDenied; Request ID: 6AB0026067A126E1; S3 Extended Request ID: RVf7+dUjy4HNdoDr/MFjr1tUZB+nSqRTsiNvydIRrTSlp51CpQ+PR5nEPMTWNMV+cFxbbASwzdo=), S3 Extended Request ID: RVf7+dUjy4HNdoDr/MFjr1tUZB+nSqRTsiNvydIRrTSlp51CpQ+PR5nEPMTWNMV+cFxbbASwzdo=\n",
      "\tat org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:158)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:101)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.mkdirs(S3AFileSystem.java:1484)\n",
      "\tat org.apache.hadoop.fs.FileSystem.mkdirs(FileSystem.java:1961)\n",
      "\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.setupJob(FileOutputCommitter.java:339)\n",
      "\tat com.amazon.emr.committer.FilterParquetOutputCommitter.setupJob(FilterParquetOutputCommitter.java:77)\n",
      "\tat com.amazon.emr.committer.EmrOptimizedSparkSqlParquetOutputCommitter.setupJob(EmrOptimizedSparkSqlParquetOutputCommitter.java:9)\n",
      "\tat org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.setupJob(HadoopMapReduceCommitProtocol.scala:162)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:139)\n",
      "\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:159)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:104)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:102)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:122)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:156)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:80)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:80)\n",
      "\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:676)\n",
      "\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:676)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:676)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:285)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:271)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:229)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:566)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: com.amazonaws.services.s3.model.AmazonS3Exception: Access Denied (Service: Amazon S3; Status Code: 403; Error Code: AccessDenied; Request ID: 6AB0026067A126E1; S3 Extended Request ID: RVf7+dUjy4HNdoDr/MFjr1tUZB+nSqRTsiNvydIRrTSlp51CpQ+PR5nEPMTWNMV+cFxbbASwzdo=), S3 Extended Request ID: RVf7+dUjy4HNdoDr/MFjr1tUZB+nSqRTsiNvydIRrTSlp51CpQ+PR5nEPMTWNMV+cFxbbASwzdo=\n",
      "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleErrorResponse(AmazonHttpClient.java:1712)\n",
      "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeOneRequest(AmazonHttpClient.java:1367)\n",
      "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeHelper(AmazonHttpClient.java:1113)\n",
      "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.doExecute(AmazonHttpClient.java:770)\n",
      "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeWithTimer(AmazonHttpClient.java:744)\n",
      "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.execute(AmazonHttpClient.java:726)\n",
      "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.access$500(AmazonHttpClient.java:686)\n",
      "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:668)\n",
      "\tat com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:532)\n",
      "\tat com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:512)\n",
      "\tat com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:4926)\n",
      "\tat com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:4872)\n",
      "\tat com.amazonaws.services.s3.AmazonS3Client.access$300(AmazonS3Client.java:390)\n",
      "\tat com.amazonaws.services.s3.AmazonS3Client$PutObjectStrategy.invokeServiceCall(AmazonS3Client.java:5806)\n",
      "\tat com.amazonaws.services.s3.AmazonS3Client.uploadObject(AmazonS3Client.java:1794)\n",
      "\tat com.amazonaws.services.s3.AmazonS3Client.putObject(AmazonS3Client.java:1754)\n",
      "\tat com.amazonaws.services.s3.transfer.internal.UploadCallable.uploadInOneChunk(UploadCallable.java:131)\n",
      "\tat com.amazonaws.services.s3.transfer.internal.UploadCallable.call(UploadCallable.java:123)\n",
      "\tat com.amazonaws.services.s3.transfer.internal.UploadMonitor.call(UploadMonitor.java:143)\n",
      "\tat com.amazonaws.services.s3.transfer.internal.UploadMonitor.call(UploadMonitor.java:48)\n",
      "\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\t... 1 more\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"<stdin>\", line 8, in main\n",
      "  File \"<stdin>\", line 36, in process_song_data\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/readwriter.py\", line 843, in parquet\n",
      "    self._jwrite.parquet(path)\n",
      "  File \"/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1257, in __call__\n",
      "    answer, self.gateway_client, self.target_id, self.name)\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/utils.py\", line 63, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\", line 328, in get_return_value\n",
      "    format(target_id, \".\", name), value)\n",
      "py4j.protocol.Py4JJavaError: An error occurred while calling o109.parquet.\n",
      ": java.nio.file.AccessDeniedException: s3a://udacity-dend/songs/_temporary/0: innerMkdirs on s3a://udacity-dend/songs/_temporary/0: com.amazonaws.services.s3.model.AmazonS3Exception: Access Denied (Service: Amazon S3; Status Code: 403; Error Code: AccessDenied; Request ID: 6AB0026067A126E1; S3 Extended Request ID: RVf7+dUjy4HNdoDr/MFjr1tUZB+nSqRTsiNvydIRrTSlp51CpQ+PR5nEPMTWNMV+cFxbbASwzdo=), S3 Extended Request ID: RVf7+dUjy4HNdoDr/MFjr1tUZB+nSqRTsiNvydIRrTSlp51CpQ+PR5nEPMTWNMV+cFxbbASwzdo=\n",
      "\tat org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:158)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:101)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.mkdirs(S3AFileSystem.java:1484)\n",
      "\tat org.apache.hadoop.fs.FileSystem.mkdirs(FileSystem.java:1961)\n",
      "\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.setupJob(FileOutputCommitter.java:339)\n",
      "\tat com.amazon.emr.committer.FilterParquetOutputCommitter.setupJob(FilterParquetOutputCommitter.java:77)\n",
      "\tat com.amazon.emr.committer.EmrOptimizedSparkSqlParquetOutputCommitter.setupJob(EmrOptimizedSparkSqlParquetOutputCommitter.java:9)\n",
      "\tat org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.setupJob(HadoopMapReduceCommitProtocol.scala:162)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:139)\n",
      "\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:159)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:104)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:102)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:122)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:156)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:80)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:80)\n",
      "\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:676)\n",
      "\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:676)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:676)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:285)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:271)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:229)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:566)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: com.amazonaws.services.s3.model.AmazonS3Exception: Access Denied (Service: Amazon S3; Status Code: 403; Error Code: AccessDenied; Request ID: 6AB0026067A126E1; S3 Extended Request ID: RVf7+dUjy4HNdoDr/MFjr1tUZB+nSqRTsiNvydIRrTSlp51CpQ+PR5nEPMTWNMV+cFxbbASwzdo=), S3 Extended Request ID: RVf7+dUjy4HNdoDr/MFjr1tUZB+nSqRTsiNvydIRrTSlp51CpQ+PR5nEPMTWNMV+cFxbbASwzdo=\n",
      "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleErrorResponse(AmazonHttpClient.java:1712)\n",
      "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeOneRequest(AmazonHttpClient.java:1367)\n",
      "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeHelper(AmazonHttpClient.java:1113)\n",
      "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.doExecute(AmazonHttpClient.java:770)\n",
      "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeWithTimer(AmazonHttpClient.java:744)\n",
      "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.execute(AmazonHttpClient.java:726)\n",
      "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.access$500(AmazonHttpClient.java:686)\n",
      "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:668)\n",
      "\tat com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:532)\n",
      "\tat com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:512)\n",
      "\tat com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:4926)\n",
      "\tat com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:4872)\n",
      "\tat com.amazonaws.services.s3.AmazonS3Client.access$300(AmazonS3Client.java:390)\n",
      "\tat com.amazonaws.services.s3.AmazonS3Client$PutObjectStrategy.invokeServiceCall(AmazonS3Client.java:5806)\n",
      "\tat com.amazonaws.services.s3.AmazonS3Client.uploadObject(AmazonS3Client.java:1794)\n",
      "\tat com.amazonaws.services.s3.AmazonS3Client.putObject(AmazonS3Client.java:1754)\n",
      "\tat com.amazonaws.services.s3.transfer.internal.UploadCallable.uploadInOneChunk(UploadCallable.java:131)\n",
      "\tat com.amazonaws.services.s3.transfer.internal.UploadCallable.call(UploadCallable.java:123)\n",
      "\tat com.amazonaws.services.s3.transfer.internal.UploadMonitor.call(UploadMonitor.java:143)\n",
      "\tat com.amazonaws.services.s3.transfer.internal.UploadMonitor.call(UploadMonitor.java:48)\n",
      "\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\t... 1 more\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
